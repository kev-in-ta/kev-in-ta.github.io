<!doctype html><html><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Inter&display=swap" rel=stylesheet><link href="https://fonts.googleapis.com/css2?family=Open+Sans:wght@400;600&display=swap" rel=stylesheet><meta name=viewport content="width=device-width,initial-scale=1"><link rel=stylesheet href=/css/home.css type=text/css media=all><link rel=stylesheet href=/css/syntax.css type=text/css media=all><link disabled id=dark-mode-theme rel=stylesheet href=/css/dark.css><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><script defer language=javascript type=text/javascript src=/js/myscripts.js></script><html><head><title>Kevin Ta</title><script src=https://use.fontawesome.com/b988afb9a9.js></script></head><link rel="shortcut icon" type=favicon href=/favicon.png></html><body><div class=dark-mode-toggle><p id=toggle class="fas fa-moon" style=font-size:35px></p></div><h1 class=home><a href=/>Kevin Ta</a></h1><div class=sub-nav><a href=/posts class=menu-item><i class="fas fa-bookmark"></i>
blog</a>
<a href=/publications class=menu-item><i class="fas fa-pen-nib"></i>
pubs</a>
<a href=/resume.pdf target=_blank class=menu-item><i class="fas fa-user-circle"></i>
resume</a>
<a href=/cv.pdf target=_blank class=menu-item><i class="fas fa-school"></i>
cv</a></div><div id=content><h2><span class=header>Publications</span></h2><div class=projects><table class=pub-info><tr><td class=pub-name>UncLe-SLAM: Uncertainty Learning for Dense Neural SLAM</td></tr></tr><td class=pub-conf><i class="fa fa-map-pin"></i> ICCV Second Workshop on Uncertainty Quantification for Computer Vision, 2023</td><tr><tr><td class=pub-authors>Kevin Ta, Erik Sandström, Luc Van Gool, and Martin R. Oswald</td></tr><tr><td class=pub-links><a class=project-link href=https://github.com/kev-in-ta/UncLe-SLAM target=_blank>[code]</a><a class=project-link href=/papers/2023iccvw-cvl.bib target=_blank>[bibtex]</a><a class=project-link href=https://arxiv.org/abs/2306.11048 target=_blank>[arxiv]</a></td></tr></table><p class=pub-abstract>We present an uncertainty learning framework for dense neural simultaneous localization and mapping (SLAM). Estimating pixel-wise uncertainties for the depth input of dense SLAM methods allows to re-weigh the tracking and mapping losses towards image regions that contain more suitable information that is more reliable for SLAM. To this end, we propose an online framework for sensor uncertainty estimation that can be trained in a self-supervised manner from only 2D input data. We further discuss the advantages of the uncertainty learning for the case of multi-sensor input. Extensive analysis, experimentation, and ablations show that our proposed modeling paradigm improves both mapping and tracking accuracy and often performs better than alternatives that require ground truth depth or 3D. Our experiments show that we achieve a 38% and 27% lower absolute trajectory tracking error (ATE) on the 7-Scenes and TUM-RGBD datasets respectively. On the popular Replica dataset on two types of depth sensors we report an 11% F1-score improvement on RGBD SLAM compared to the recent state-of-the-art neural implicit approaches.</p><table class=pub-info><tr><td class=pub-name>L2E: Lasers to Events for 6-DoF Extrinsic Calibration of Lidars and Event Cameras</td></tr></tr><td class=pub-conf><i class="fa fa-map-pin"></i> International Conference on Robotics and Automation (ICRA), 2023</td><tr><tr><td class=pub-authors>Kevin Ta, David Bruggemann, Tim Brödermann, Christos Sakaridis, and Luc Van Gool</td></tr><tr><td class=pub-links><a class=project-link href=https://ieeexplore.ieee.org/document/10161220 target=_blank>[paper]</a><a class=project-link href=https://github.com/kev-in-ta/l2e target=_blank>[code]</a><a class=project-link href=/papers/2023icra-cvl.bib target=_blank>[bibtex]</a><a class=project-link href=https://arxiv.org/abs/2207.01009 target=_blank>[arxiv]</a></td></tr></table><p class=pub-abstract>As neuromorphic technology is maturing, its application to robotics and autonomous vehicle systems has become an area of active research. In particular, event cameras have emerged as a compelling alternative to frame-based cameras in low-power and latency-demanding applications. To enable event cameras to operate alongside staple sensors like lidar in perception tasks, we propose a direct, temporally-decoupled extrinsic calibration method between event cameras and lidars. The high dynamic range, high temporal resolution, and low-latency operation of event cameras are exploited to directly register lidar laser returns, allowing information-based correlation methods to optimize for the 6-DoF extrinsic calibration between the two sensors. This paper presents the first direct calibration method between event cameras and lidars, removing dependencies on frame-based camera intermediaries and/or highly-accurate hand measurements.</p><table class=pub-info><tr><td class=pub-name>ATTENTIV: Instrumented Peripheral Catheter for the Detection of Catheter Dislodgement in IV Infiltration</td></tr></tr><td class=pub-conf><i class="fa fa-map-pin"></i> International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), 2022</td><tr><tr><td class=pub-authors>Jessica Y. Bo, Kevin Ta, Rio Nishida, Gordon Yeh, Vivian W. L. Tsang, Megan Bolton, Manon Ranger, and Konrad Walus</td></tr><tr><td class=pub-links><a class=project-link href=https://ieeexplore.ieee.org/document/9871840 target=_blank>[paper]</a><a class=project-link href=https://github.com/jessica-bo/bioimpedance_svm target=_blank>[code]</a><a class=project-link href=/papers/2022embc-attentiv.bib target=_blank>[bibtex]</a></td></tr></table><p class=pub-abstract>Intravenous (IV) infiltration is a common problem associated with IV infusion therapy in clinical practice. A multitude of factors can cause the leakage of IV fluids into the surrounding tissues, resulting in symptoms ranging from temporary swelling to permanent tissue damage. Severe infiltration outcomes can be avoided or minimized if the patient's care provider is alerted of the infiltration at its earliest onset. However, there is a lack of real-time, continuous infiltration monitoring solutions, especially those suited for clinical use for critically ill patients. Our design of the sensor-integrated ATTENTIV catheter allows direct detection of catheter dislodgement, a root cause of IV infiltration. We verify two detection methods: blood-tissue differentiation with a support vector machine and signal peak identification with a thresholding algorithm. We present promising preliminary testing results on biological and phantom models that utilize bioimpedance as the sensing modality. Clinical relevance- The sensor-embedded ATTENTIV catheter demonstrates potential to automate IV infiltration detection in lieu of using traditional infusion catheters and manual detection methods.</p><table class=pub-info><tr><td class=pub-name>Offline and Real-Time Implementation of a Terrain Classification Pipeline for Pushrim-Activated Power-Assisted Wheelchairs</td></tr></tr><td class=pub-conf><i class="fa fa-map-pin"></i> International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), 2021</td><tr><tr><td class=pub-authors>Mahsa Khalili, Kevin Ta, H. F. Machiel Van der Loos, and Jaimie F. Borisoff</td></tr><tr><td class=pub-links><a class=project-link href=https://ieeexplore.ieee.org/document/9630749 target=_blank>[paper]</a><a class=project-link href=https://github.com/kev-in-ta/CARIS-PAW-RT-terrain-classification target=_blank>[code]</a><a class=project-link href=/papers/2021embc-caris.bib target=_blank>[bibtex]</a></td></tr></table><p class=pub-abstract>Pushrim-activated power-assisted wheelchairs (PAPAWs) are assistive technologies that provide propulsion assist to wheelchair users and enable access to various indoor and outdoor terrains. Therefore, it is beneficial to use PAPAW controllers that adapt to different terrain conditions. To achieve this objective, terrain classification techniques can be used as an integral part of the control architecture. Previously, the feasibility of using learning-based terrain classification models was investigated for offline applications. In this paper, we examine the effects of three model parameters (i.e., feature characteristics, terrain types, and the length of data segments) on offline and real-time classification accuracy. Our findings revealed that Random Forest classifiers are computationally efficient and can be used effectively for real-time terrain classification. These classifiers have the highest performance accuracy when used with a combination of time- and frequency-domain features. Additionally, we found that increasing the number of data points used for terrain estimation improves the prediction accuracy. Finally, our results revealed that classification accuracy can be improved by considering terrains with similar characteristics under one umbrella group. These findings can contribute to the development of real-time adaptive controllers that enhance PAPAW usability on different terrains.</p><table class=pub-info><tr><td class=pub-name>Offline and Real-Time Implementation of a Personalized Wheelchair User Intention Detection Pipeline: A Case Study</td></tr></tr><td class=pub-conf><i class="fa fa-map-pin"></i> International Conference on Robot and Human Interactive Communication (ROMAN), 2021</td><tr><tr><td class=pub-authors>Mahsa Khalili, Kevin Ta, H. F. Machiel Van der Loos, and Jaimie F. Borisoff</td></tr><tr><td class=pub-links><a class=project-link href=https://ieeexplore.ieee.org/document/9515488 target=_blank>[paper]</a><a class=project-link href=https://github.com/kev-in-ta/CARIS-PAW-RT-intention-detection target=_blank>[code]</a><a class=project-link href=/papers/2021roman-caris.bib target=_blank>[bibtex]</a></td></tr></table><p class=pub-abstract>Pushrim-activated power-assisted wheels (PAPAWs) are assistive technologies that provide on-demand assistance to wheelchair users. PAPAWs operate based on a collaborative control scheme and require an accurate interpretation of the user’s intent to provide effective propulsion assistance. This paper investigates a user-specific intention estimation framework for wheelchair users. We used Gaussian Mixture models (GMM) to identify implicit intentions from user-pushrim interactions (i.e., input torque to the pushrims). Six clusters emerged that were associated with different phases of a stroke pattern and the intention about the desired direction of motion. GMM predictions were used as 'ground truth' labels for further intention estimation analysis. Next, Random Forest (RF) classifiers were trained to predict user intentions. The best optimal classifier had an overall prediction accuracy of 94.7%. Finally, a Bayesian filtering (BF) algorithm was used to extract sequential dependencies of the user-pushrim measurements. The BF algorithm improved sequences of intention predictions for some wheelchair maneuvers compared to the GMM and RF predictions. The proposed intention estimation pipeline is computationally efficient and was successfully tested and used for real-time prediction of wheelchair user’s intentions. This framework provides the foundation for the development of user-specific and adaptive PAPAW controllers.</p><table class=pub-info><tr><td class=pub-name>Development of A Learning-Based Terrain Classification Framework for Pushrim-Activated Power-Assisted Wheelchairs</td></tr></tr><td class=pub-conf><i class="fa fa-map-pin"></i> International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), 2020</td><tr><tr><td class=pub-authors>Mahsa Khalili, Keenan T. McConkey, Kevin Ta, Lyndia C. Wu, H.F. Machiel Van der Loos, and Jaimie F. Borisoff</td></tr><tr><td class=pub-links><a class=project-link href=https://ieeexplore.ieee.org/document/9175678 target=_blank>[paper]</a><a class=project-link href=https://github.com/kev-in-ta/CARIS-PAW-RT-data-acquisition target=_blank>[code]</a><a class=project-link href=/papers/2020embc-caris.bib target=_blank>[bibtex]</a></td></tr></table><p class=pub-abstract>Pushrim-activated power-assisted wheels (PAPAWs) are assistive technologies that provide on-demand torque assistance to wheelchair users. Although the available power can reduce the physical load of wheelchair propulsion, it may also cause maneuverability and controllability issues. Commercially-available PAPAW controllers are insensitive to environmental changes, leading to inefficient and/or unsafe wheelchair movements. In this regard, adaptive velocity/torque control strategies could be employed to improve safety and stability. To investigate this objective, we propose a context-aware sensory framework to recognize terrain conditions. In this paper, we present a learning-based terrain classification framework for PAPAWs. Study participants performed various maneuvers consisting of common daily-life wheelchair propulsion routines on different indoor and outdoor terrains. Relevant features from wheelchair frame-mounted gyroscope and accelerometer measurements were extracted and used to train and test the proposed classifiers. Our findings revealed that a one-stage multi-label classification framework has a higher accuracy performance compared to a two-stage classification pipeline with an indoor-outdoor classification in the first stage. We also found that, on average, outdoor terrains can be classified with higher accuracy (90%) compared to indoor terrains (65%). This framework can be used for real-time terrain classification applications and provide the required information for an adaptive velocity/torque controller design.</p></div></div><p class=copyright>Kevin Ta © 2022</p><p class=footer-links><a class=source href=https://github.com/kev-in-ta/kev-in-ta.github.io target=_blank><i class="fas fa-code"></i></a>
<a class=source href=/posts/index.xml target=_blank><i class="fas fa-rss-square"></i></a>
<a class=source href=https://github.com/kev-in-ta target=_blank><i class="fab fa-github"></i></a>
<a class=source href=https://www.linkedin.com/in/kevinjhyta target=_blank><i class="fab fa-linkedin-in"></i></a>
<a class=source href="https://scholar.google.com/citations?user=npFtagoAAAAJ&amp;hl=en" target=_blank><i class="fas fa-graduation-cap"></i></a>
<a class=source href=https://instagram.com/kta.climbs target=_blank><i class="fab fa-instagram"></i></a></p></body></html>